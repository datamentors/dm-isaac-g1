# Training Configuration
# Based on isaac-g1-ulc-vlm curriculum approach

training:
  # General settings
  seed: 42
  num_envs: 4096
  max_iterations: 10000

  # PPO hyperparameters
  ppo:
    learning_rate: 3.0e-4
    gamma: 0.99
    lam: 0.95
    clip_param: 0.2
    num_epochs: 5
    num_mini_batches: 4
    value_loss_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 1.0

  # Dual Actor-Critic architecture
  policy:
    # Locomotion branch
    locomotion:
      observation_dim: 57
      action_dim: 12  # leg joints
      hidden_dims: [256, 256, 128]
      activation: "elu"

    # Arm branch
    arm:
      observation_dim: 55
      action_dim: 5  # arm joints
      hidden_dims: [256, 256, 128]
      activation: "elu"

  # Curriculum stages
  curriculum:
    stage_1:
      name: "standing"
      iterations: 1000
      rewards:
        base_height: 1.0
        orientation: 0.5
        joint_regularization: 0.1

    stage_2:
      name: "locomotion"
      iterations: 2000
      rewards:
        velocity_tracking: 1.0
        orientation: 0.5
        energy: -0.01

    stage_3:
      name: "torso_control"
      iterations: 1500
      rewards:
        torso_height: 1.0
        torso_orientation: 0.5

    stage_4:
      name: "fixed_base_reaching"
      iterations: 2000
      rewards:
        reaching_distance: 1.0
        end_effector_orientation: 0.3

    stage_5:
      name: "dynamic_arm_reaching"
      iterations: 2000
      rewards:
        reaching_distance: 1.0
        balance: 0.5

    stage_6:
      name: "loco_manipulation"
      iterations: 2500
      rewards:
        reaching_distance: 1.0
        velocity_tracking: 0.5
        balance: 0.5

    stage_7:
      name: "anti_gaming_reaching"
      iterations: 3000
      rewards:
        reaching_distance: 1.0
        movement_incentive: 0.3
        end_effector_displacement: 0.2
      anti_gaming:
        absolute_only_targets: true
        multi_condition_validation: true
        movement_centric_shaping: true

  # Checkpointing
  checkpoint:
    save_interval: 500
    keep_last_n: 5
    save_best: true

  # Logging
  logging:
    tensorboard: true
    wandb: true
    log_interval: 10
